{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HrrMCRX4v-dt",
        "outputId": "a83ae3a4-68f4-400d-87b4-2c99b2b346e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     Math  Reading  Writing\n",
            "0      48       68       63\n",
            "1      62       81       72\n",
            "2      79       80       78\n",
            "3      76       83       79\n",
            "4      59       64       62\n",
            "..    ...      ...      ...\n",
            "995    72       74       70\n",
            "996    73       86       90\n",
            "997    89       87       94\n",
            "998    83       82       78\n",
            "999    66       66       72\n",
            "\n",
            "[1000 rows x 3 columns]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "data = pd.read_csv('/content/drive/MyDrive/student.csv')\n",
        "print(data)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Top 5 rows of dataset:\")\n",
        "print(data.head())\n",
        "\n",
        "print(\"Top 5 columns of dataset:\")\n",
        "print(data.tail())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PUk7W-3vyg9O",
        "outputId": "58a2e97f-34f7-4c74-b2bc-378dcbb4c57a"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 rows of dataset:\n",
            "   Math  Reading  Writing\n",
            "0    48       68       63\n",
            "1    62       81       72\n",
            "2    79       80       78\n",
            "3    76       83       79\n",
            "4    59       64       62\n",
            "Top 5 columns of dataset:\n",
            "     Math  Reading  Writing\n",
            "995    72       74       70\n",
            "996    73       86       90\n",
            "997    89       87       94\n",
            "998    83       82       78\n",
            "999    66       66       72\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Dataset information\")\n",
        "data.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D0L2Cstjy6hU",
        "outputId": "7b705e75-bebc-46f7-985f-37898079d940"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset information\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1000 entries, 0 to 999\n",
            "Data columns (total 3 columns):\n",
            " #   Column   Non-Null Count  Dtype\n",
            "---  ------   --------------  -----\n",
            " 0   Math     1000 non-null   int64\n",
            " 1   Reading  1000 non-null   int64\n",
            " 2   Writing  1000 non-null   int64\n",
            "dtypes: int64(3)\n",
            "memory usage: 23.6 KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"descriptive info:\")\n",
        "print(data.describe())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJ_xZQeizIHS",
        "outputId": "d4ce9d35-abe0-4d2b-b379-76b6c83d8a54"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "descriptive info:\n",
            "              Math      Reading      Writing\n",
            "count  1000.000000  1000.000000  1000.000000\n",
            "mean     67.290000    69.872000    68.616000\n",
            "std      15.085008    14.657027    15.241287\n",
            "min      13.000000    19.000000    14.000000\n",
            "25%      58.000000    60.750000    58.000000\n",
            "50%      68.000000    70.000000    69.500000\n",
            "75%      78.000000    81.000000    79.000000\n",
            "max     100.000000   100.000000   100.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = data[['Math', 'Reading']]\n",
        "Y = data['Writing']\n",
        "\n",
        "print(\"Feature X :\\n\", X.head())\n",
        "print(\"Label Y :\\n\", Y.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mwoeQF9ZzR8_",
        "outputId": "b357ff8c-0c5a-4ede-b8e6-d28ab96e71e5"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature X :\n",
            "    Math  Reading\n",
            "0    48       68\n",
            "1    62       81\n",
            "2    79       80\n",
            "3    76       83\n",
            "4    59       64\n",
            "Label Y :\n",
            " 0    63\n",
            "1    72\n",
            "2    78\n",
            "3    79\n",
            "4    62\n",
            "Name: Writing, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "d = 2\n",
        "n = 5\n",
        "\n",
        "W = np.random.rand(d, 1)\n",
        "X_synthetic = np.random.rand(d, n)\n",
        "\n",
        "Y_synthetic = np.dot(W.T, X_synthetic)\n",
        "\n",
        "print(\"Weight Vector (W):\\n\", W)\n",
        "print(\"\\nFeature Matrix (X):\\n\", X_synthetic)\n",
        "print(\"\\nPredicted Values (Y):\\n\", Y_synthetic)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7MGsY2VC_9q0",
        "outputId": "0d0fef85-1c39-4257-e3aa-66a6ed591f3a"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weight Vector (W):\n",
            " [[0.94530153]\n",
            " [0.86948853]]\n",
            "\n",
            "Feature Matrix (X):\n",
            " [[0.4541624  0.32670088 0.23274413 0.61446471 0.03307459]\n",
            " [0.01560606 0.42879572 0.06807407 0.25194099 0.22116092]]\n",
            "\n",
            "Predicted Values (Y):\n",
            " [[0.4428897  0.68166381 0.27920301 0.79991423 0.22356234]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = data[['Math', 'Reading']]\n",
        "Y = data['Writing']\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
        "\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"X_test shape:\", X_test.shape)\n",
        "print(\"Y_train shape:\", Y_train.shape)\n",
        "print(\"Y_test shape:\", Y_test.shape)"
      ],
      "metadata": {
        "id": "0Gf7brbPF3gN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7249805d-d831-43be-9f62-dd19946cb234"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape: (700, 2)\n",
            "X_test shape: (300, 2)\n",
            "Y_train shape: (700,)\n",
            "Y_test shape: (300,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def cost_function(X, Y, W):\n",
        "    ypred = np.dot(X, W)\n",
        "\n",
        "    squared_error = np.square(ypred - Y)\n",
        "\n",
        "    n = len(Y)\n",
        "    cost = np.sum(squared_error) / (2 * n)\n",
        "\n",
        "    return cost\n"
      ],
      "metadata": {
        "id": "eyDtexc7CP4_"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = np.array([[1, 2], [3, 4], [5, 6]])\n",
        "Y_test = np.array([3, 7, 11])\n",
        "W_test = np.array([1, 1])\n",
        "\n",
        "cost = cost_function(X_test, Y_test, W_test)\n",
        "\n",
        "if cost == 0:\n",
        "    print(\"Proceed Further\")\n",
        "else:\n",
        "    print(\"Something went wrong: Reimplement the cost function\")\n",
        "\n",
        "print(\"Cost function output:\", cost)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ks1D4o88DANS",
        "outputId": "dea4d58c-52a7-4bcb-a28a-e727e1689a8c"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Proceed Further\n",
            "Cost function output: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(X, Y, W, alpha, iterations):\n",
        "    cost_history = []\n",
        "    m = len(Y)\n",
        "\n",
        "    for _ in range(iterations):\n",
        "        Y_pred = np.dot(X, W)\n",
        "\n",
        "        loss = Y_pred - Y\n",
        "\n",
        "        dw = (1 / m) * np.dot(X.T, loss)\n",
        "\n",
        "        W -= alpha * dw\n",
        "\n",
        "        cost = cost_function(X, Y, W)\n",
        "        cost_history.append(cost)\n",
        "\n",
        "    return W, cost_history"
      ],
      "metadata": {
        "id": "1YtshLWEJG2f"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(0)\n",
        "X = np.random.rand(100, 3)\n",
        "Y = np.random.rand(100)\n",
        "W = np.random.rand(3)\n",
        "\n",
        "alpha = 0.01\n",
        "iterations = 1000\n",
        "\n",
        "final_params, cost_history = gradient_descent(X, Y, W, alpha, iterations)\n",
        "\n",
        "print(\"Final Parameters:\", final_params)\n",
        "print(\"Cost History:\", cost_history)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rlIk9gLXJSYr",
        "outputId": "2b758a56-4981-4662-d8b0-fc2d9d33321e"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Parameters: [0.20551667 0.54295081 0.10388027]\n",
            "Cost History: [0.10711197094660153, 0.10634880599939901, 0.10559826315680616, 0.10486012948320558, 0.1041341956428534, 0.10342025583900626, 0.1027181077540776, 0.1020275524908062, 0.10134839451441931, 0.1006804415957737, 0.1000235047554587, 0.09937739820884377, 0.09874193931205609, 0.09811694850887098, 0.09750224927850094, 0.0968976680842672, 0.09630303432313951, 0.09571818027612913, 0.09514294105952065, 0.09457715457692842, 0.09402066147216397, 0.09347330508290015, 0.09293493139511913, 0.09240538899833017, 0.09188452904154543, 0.0913722051899995, 0.09086827358260123, 0.09037259279010502, 0.08988502377398917, 0.08940542984603007, 0.08893367662855953, 0.08846963201539432, 0.08801316613342668, 0.08756415130486386, 0.08712246201010665, 0.08668797485125507, 0.08626056851623205, 0.08584012374351278, 0.08542652328745133, 0.08501965188419301, 0.0846193962181636, 0.08422564488912489, 0.08383828837978763, 0.08345721902397185, 0.08308233097530582, 0.08271352017645425, 0.08235068432886682, 0.08199372286303816, 0.08164253690927113, 0.08129702926893387, 0.08095710438620353, 0.08062266832028739, 0.08029362871811391, 0.07996989478748553, 0.0796513772706855, 0.07933798841853087, 0.07902964196486459, 0.07872625310147845, 0.07842773845346054, 0.07813401605495937, 0.0778450053253578, 0.0775606270458499, 0.07728080333641404, 0.07700545763317514, 0.07673451466614989, 0.07646790043736812, 0.07620554219936448, 0.07594736843403344, 0.07569330883184205, 0.07544329427139428, 0.07519725679934072, 0.0749551296106282, 0.07471684702908327, 0.07448234448832412, 0.0742515585129952, 0.07402442670031911, 0.0738008877019607, 0.07358088120619749, 0.0733643479203919, 0.07315122955375959, 0.07294146880042966, 0.07273500932279067, 0.07253179573511871, 0.07233177358748233, 0.0721348893499193, 0.07194109039688139, 0.07175032499194182, 0.07156254227276149, 0.07137769223630935, 0.07119572572433286, 0.07101659440907385, 0.07084025077922623, 0.070666648126131, 0.07049574053020462, 0.07032748284759716, 0.07016183069707572, 0.0699987404471299, 0.06983816920329523, 0.06968007479569092, 0.06952441576676843, 0.06937115135926715, 0.06922024150437375, 0.06907164681008185, 0.06892532854974835, 0.0687812486508435, 0.06863936968389095, 0.06849965485159508, 0.06836206797815195, 0.06822657349874123, 0.06809313644919561, 0.067961722455845, 0.06783229772553254, 0.06770482903579932, 0.06757928372523506, 0.06745562968399212, 0.06733383534445969, 0.06721386967209597, 0.067095702156415, 0.06697930280212627, 0.06686464212042395, 0.06675169112042348, 0.0666404213007429, 0.06653080464122665, 0.06642281359480932, 0.06631642107951677, 0.06621160047060279, 0.06610832559281864, 0.06600657071281309, 0.0659063105316614, 0.06580752017752023, 0.06571017519840698, 0.06561425155510119, 0.06551972561416586, 0.06542657414108709, 0.06533477429352925, 0.06524430361470467, 0.06515514002685512, 0.06506726182484374, 0.06498064766985515, 0.06489527658320228, 0.06481112794023773, 0.06472818146436811, 0.0646464172211699, 0.06456581561260431, 0.06448635737133043, 0.0644080235551142, 0.06433079554133217, 0.06425465502156798, 0.06417958399630046, 0.06410556476968135, 0.06403257994440141, 0.0639606124166433, 0.06388964537111992, 0.06381966227619645, 0.06375064687909507, 0.06368258320118075, 0.06361545553332655, 0.06354924843135755, 0.06348394671157162, 0.06341953544633615, 0.06335599995975896, 0.06329332582343267, 0.06323149885225086, 0.06317050510029515, 0.06311033085679153, 0.06305096264213547, 0.06299238720398384, 0.0629345915134133, 0.06287756276114324, 0.06282128835382297, 0.0627657559103815, 0.06271095325843898, 0.06265686843077901, 0.06260348966188052, 0.06255080538450809, 0.06249880422636036, 0.06244747500677472, 0.06239680673348793, 0.06234678859945137, 0.06229740997970036, 0.06224866042827619, 0.06220052967520031, 0.062153007623499706, 0.062106084346282515, 0.06205975008386309, 0.06201399524093575, 0.06196881038379625, 0.061924186237610215, 0.061880113683727866, 0.0618365837570441, 0.06179358764340313, 0.061751116677047156, 0.06170916233810801, 0.0616677162501414, 0.06162677017770278, 0.061586316023964055, 0.0615463458283708, 0.06150685176433905, 0.06146782613699094, 0.0614292613809287, 0.061391150058046254, 0.06135348485537794, 0.06131625858298352, 0.061279464171868706, 0.06124309467194143, 0.061207143250002184, 0.0611716031877684, 0.06113646787993252, 0.061101730832252524, 0.06106738565967507, 0.06103342608449018, 0.06099984593451716, 0.06096663914132128, 0.0609337997384604, 0.0609013218597616, 0.06086919973762659, 0.06083742770136588, 0.06080600017556133, 0.06077491167845611, 0.06074415682037193, 0.060713730302153254, 0.060683626913637524, 0.060653841532151406, 0.060624369121032556, 0.0605952047281761, 0.06056634348460599, 0.060537780603070336, 0.06050951137666054, 0.0604815311774538, 0.060453835455178496, 0.06042641973590228, 0.06039927962074215, 0.060372410784596583, 0.060345808974898815, 0.06031947001039151, 0.06029338977992186, 0.06026756424125725, 0.060241989419920934, 0.06021666140804729, 0.0601915763632565, 0.06016673050754826, 0.060142120126214255, 0.06011774156676883, 0.06009359123789796, 0.06006966560842588, 0.06004596120629915, 0.060022474617588105, 0.059999202485504784, 0.059976141509438, 0.0599532884440042, 0.05993064009811483, 0.05990819333405906, 0.059885945066602345, 0.059863892262100066, 0.059842031937626106, 0.059820361160116395, 0.05979887704552664, 0.05977757675800453, 0.05975645750907579, 0.05973551655684408, 0.0597147512052044, 0.05969415880306974, 0.05967373674361096, 0.05965348246350928, 0.05963339344222168, 0.059613467201258485, 0.059593701303473294, 0.05957409335236496, 0.0595546409913911, 0.05953534190329372, 0.05951619380943562, 0.0594971944691485, 0.0594783416790919, 0.05945963327262296, 0.0594410671191769, 0.05942264112365792, 0.05940435322584049, 0.059386201399780576, 0.059368183653237094, 0.059350298027102844, 0.05933254259484532, 0.05931491546195686, 0.05929741476541398, 0.0592800386731462, 0.05926278538351338, 0.05924565312479226, 0.05922864015467153, 0.059211744759755505, 0.05919496525507604, 0.05917829998361292, 0.05916174731582211, 0.059145305649172315, 0.059128973407688926, 0.05911274904150609, 0.05909663102642617, 0.05908061786348662, 0.059064708078534194, 0.05904890022180654, 0.05903319286752055, 0.05901758461346795, 0.05900207408061755, 0.058986659912724324, 0.05897134077594504, 0.058956115358460404, 0.05894098237010357, 0.05892594054199501, 0.05891098862618344, 0.05889612539529293, 0.05888134964217588, 0.05886666017957195, 0.058852055839772675, 0.05883753547429179, 0.05882309795354117, 0.058808742166512155, 0.05879446702046235, 0.058780271440607684, 0.058766154369819606, 0.05875211476832761, 0.05873815161342641, 0.05872426389918856, 0.058710450636181515, 0.05869671085118971, 0.058683043586941104, 0.058669447901838714, 0.05865592286969638, 0.05864246757947903, 0.05862908113504752, 0.05861576265490756, 0.058602511271963004, 0.05858932613327336, 0.058576206399815284, 0.05856315124624814, 0.058550159860683564, 0.058537231444458875, 0.05852436521191438, 0.05851156039017436, 0.0584988162189318, 0.05848613195023677, 0.05847350684828838, 0.058460940189230176, 0.05844843126094919, 0.05843597936287806, 0.05842358380580092, 0.05841124391166213, 0.058398959013378576, 0.05838672845465502, 0.05837455158980245, 0.05836242778355972, 0.05835035641091811, 0.05833833685694878, 0.05832636851663321, 0.05831445079469654, 0.058302583105443666, 0.05829076487259809, 0.05827899552914358, 0.05826727451716844, 0.058255601287712386, 0.0582439753006161, 0.058232396024373266, 0.05822086293598511, 0.058209375520817355, 0.058197933272459756, 0.05818653569258779, 0.05817518229082684, 0.058163872584618616, 0.05815260609908985, 0.058141382366923164, 0.058130200928230166, 0.058119061330426776, 0.05810796312811039, 0.05809690588293942, 0.05808588916351474, 0.058074912545263056, 0.058063975610322414, 0.058053077947429504, 0.05804221915180897, 0.05803139882506447, 0.05802061657507169, 0.0580098720158732, 0.05799916476757483, 0.057988494456244134, 0.057977860713810364, 0.057967263177966154, 0.05795670149207094, 0.05794617530505593, 0.05793568427133074, 0.057925228050691516, 0.057914806308230836, 0.057904418714248757, 0.057894064944165734, 0.0578837446784368, 0.05787345760246728, 0.05786320340652982, 0.05785298178568306, 0.05784279243969133, 0.057832635072946, 0.05782250939438805, 0.0578124151174319, 0.05780235195989063, 0.057792319643902336, 0.05778231789585793, 0.0577723464463298, 0.05776240503000217, 0.05775249338560223, 0.05774261125583255, 0.05773275838730473, 0.05772293453047407, 0.057713139439575324, 0.057703372872559694, 0.05769363459103265, 0.057683924360193005, 0.05767424194877295, 0.05766458712897906, 0.05765495967643433, 0.057645359370121226, 0.05763578599232564, 0.057626239328581796, 0.05761671916761811, 0.057607225301304, 0.05759775752459752, 0.057588315635493874, 0.057578899434974906, 0.05756950872695933, 0.057560143318253806, 0.05755080301850501, 0.057541487640152184, 0.05753219699838088, 0.057522930911077144, 0.05751368919878268, 0.05750447168465076, 0.05749527819440267, 0.057486108556285255, 0.05747696260102877, 0.05746784016180581, 0.05745874107419066, 0.05744966517611951, 0.057440612307851226, 0.05743158231192885, 0.05742257503314173, 0.057413590318488285, 0.05740462801713937, 0.05739568798040233, 0.057386770061685605, 0.05737787411646401, 0.05736900000224435, 0.05736014757853204, 0.05735131670679789, 0.057342507250445686, 0.05733371907478018, 0.05732495204697581, 0.05731620603604562, 0.057307480912811105, 0.057298776549872255, 0.05729009282157824, 0.05728142960399854, 0.05727278677489465, 0.05726416421369212, 0.05725556180145319, 0.05724697942084986, 0.0572384169561373, 0.05722987429312795, 0.05722135131916572, 0.05721284792310102, 0.05720436399526589, 0.0571958994274496, 0.057187454112874896, 0.057179027946174334, 0.05717062082336728, 0.057162232641836994, 0.05715386330030848, 0.05714551269882637, 0.05713718073873334, 0.05712886732264895, 0.05712057235444866, 0.05711229573924336, 0.05710403738335914, 0.0570957971943175, 0.057087575080815786, 0.05707937095270803, 0.057071184720986066, 0.05706301629776107, 0.05705486559624517, 0.057046732530733744, 0.057038617016587564, 0.05703051897021566, 0.05702243830905817, 0.057014374951569684, 0.057006328817202634, 0.05699829982639134, 0.05699028790053585, 0.05698229296198645, 0.05697431493402824, 0.056966353740865984, 0.05695840930760929, 0.056950481560257914, 0.0569425704256875, 0.0569346758316353, 0.05692679770668645, 0.05691893598026014, 0.05691109058259633, 0.05690326144474244, 0.05689544849854041, 0.056887651676613984, 0.05687987091235604, 0.056872106139916355, 0.05686435729418944, 0.05685662431080263, 0.0568489071261043, 0.05684120567715238, 0.056833519901703065, 0.05682584973819958, 0.05681819512576124, 0.05681055600417275, 0.056802932313873525, 0.056795323995947306, 0.056787730992111936, 0.05678015324470926, 0.05677259069669528, 0.05676504329163035, 0.05675751097366966, 0.05674999368755382, 0.05674249137859953, 0.05673500399269066, 0.05672753147626912, 0.056720073776326194, 0.05671263084039382, 0.056705202616536096, 0.05669778905334098, 0.05669039009991206, 0.05668300570586036, 0.05667563582129657, 0.056668280396823104, 0.05666093938352648, 0.05665361273296975, 0.056646300397185066, 0.05663900232866641, 0.05663171848036241, 0.05662444880566922, 0.05661719325842369, 0.056609951792896414, 0.056602724363785134, 0.05659551092620811, 0.05658831143569758, 0.05658112584819342, 0.05657395412003692, 0.05656679620796451, 0.05655965206910183, 0.05655252166095763, 0.056545404941418, 0.056538301868740586, 0.05653121240154893, 0.056524136498826864, 0.056517074119913024, 0.056510025224495546, 0.05650298977260663, 0.05649596772461748, 0.056488959041233064, 0.05648196368348717, 0.05647498161273735, 0.05646801279066009, 0.056461057179246134, 0.05645411474079556, 0.05644718543791332, 0.05644026923350467, 0.056433366090770515, 0.05642647597320331, 0.05641959884458242, 0.05641273466897008, 0.05640588341070717, 0.05639904503440896, 0.05639221950496121, 0.05638540678751615, 0.05637860684748858, 0.056371819650551894, 0.05636504516263454, 0.05635828334991602, 0.05635153417882347, 0.056344797616027884, 0.056338073628440656, 0.05633136218321008, 0.056324663247717996, 0.05631797678957624, 0.05631130277662355, 0.05630464117692215, 0.056297991958754665, 0.05629135509062088, 0.05628473054123466, 0.05627811827952098, 0.05627151827461283, 0.0562649304958483, 0.05625835491276777, 0.05625179149511093, 0.056245240212814004, 0.05623870103600713, 0.05623217393501148, 0.05622565888033667, 0.05621915584267811, 0.05621266479291449, 0.056206185702105234, 0.05619971854148787, 0.05619326328247578, 0.05618681989665565, 0.05618038835578517, 0.056173968631790624, 0.05616756069676472, 0.056161164522964185, 0.05615478008280768, 0.05614840734887347, 0.05614204629389748, 0.056135696890770956, 0.0561293591125386, 0.056123032932396344, 0.05611671832368947, 0.05611041525991056, 0.05610412371469757, 0.05609784366183199, 0.05609157507523687, 0.05608531792897493, 0.05607907219724691, 0.056072837854389615, 0.05606661487487427, 0.056060403233304724, 0.056054202904415734, 0.05604801386307135, 0.056041836084263226, 0.05603566954310899, 0.05602951421485069, 0.05602337007485318, 0.05601723709860265, 0.05601111526170498, 0.05600500453988435, 0.05599890490898177, 0.05599281634495359, 0.055986738823870105, 0.055980672321914095, 0.055974616815379595, 0.055968572280670384, 0.05596253869429871, 0.05595651603288404, 0.05595050427315166, 0.0559445033919315, 0.05593851336615684, 0.055932534172863126, 0.05592656578918666, 0.05592060819236353, 0.05591466135972839, 0.05590872526871329, 0.05590279989684662, 0.05589688522175184, 0.055890981221146614, 0.05588508787284151, 0.055879205154739084, 0.05587333304483278, 0.05586747152120588, 0.055861620562030555, 0.05585578014556684, 0.05584995025016162, 0.05584413085424776, 0.05583832193634303, 0.055832523475049294, 0.0558267354490515, 0.05582095783711688, 0.05581519061809395, 0.05580943377091164, 0.05580368727457861, 0.05579795110818212, 0.055792225250887444, 0.055786509681936956, 0.055780804380649245, 0.05577510932641847, 0.05576942449871345, 0.055763749877076975, 0.05575808544112503, 0.05575243117054599, 0.05574678704509999, 0.05574115304461813, 0.055735529149001775, 0.055729915338221844, 0.05572431159231819, 0.05571871789139883, 0.05571313421563932, 0.05570756054528211, 0.05570199686063586, 0.055696443142074864, 0.055690899370038335, 0.05568536552502988, 0.05567984158761686, 0.055674327538429685, 0.05566882335816142, 0.055663329027567085, 0.055657844527463085, 0.05565236983872667, 0.05564690494229538, 0.0556414498191665, 0.05563600445039652, 0.055630568817100635, 0.05562514290045211, 0.05561972668168197, 0.05561432014207832, 0.05560892326298588, 0.05560353602580557, 0.055598158411993996, 0.055592790403062906, 0.055587431980578784, 0.055582083126162425, 0.05557674382148841, 0.055571414048284674, 0.05556609378833211, 0.055560783023464094, 0.05555548173556606, 0.0555501899065751, 0.05554490751847952, 0.05553963455331848, 0.05553437099318152, 0.055529116820208266, 0.05552387201658792, 0.055518636564558965, 0.05551341044640875, 0.05550819364447313, 0.05550298614113609, 0.05549778791882936, 0.05549259896003212, 0.05548741924727061, 0.05548224876311773, 0.055477087490192804, 0.0554719354111612, 0.055466792508733924, 0.05546165876566746, 0.055456534164763226, 0.05545141868886745, 0.05544631232087083, 0.05544121504370806, 0.055436126840357744, 0.055431047693841926, 0.05542597758722593, 0.055420916503617974, 0.05541586442616892, 0.055410821338071965, 0.05540578722256242, 0.05540076206291734, 0.055395745842455345, 0.05539073854453634, 0.055385740152561175, 0.05538075064997147, 0.055375770020249314, 0.05537079824691705, 0.055365835313536935, 0.05536088120371103, 0.05535593590108084, 0.055350999389327124, 0.055346071652169704, 0.0553411526733671, 0.05533624243671647, 0.05533134092605322, 0.055326448125250914, 0.05532156401822096, 0.05531668858891247, 0.055311821821311946, 0.055306963699443185, 0.05530211420736701, 0.055297273329180996, 0.05529244104901939, 0.0552876173510529, 0.05528280221948834, 0.05527799563856866, 0.055273197592572584, 0.05526840806581448, 0.055263627042644155, 0.055258854507446706, 0.05525409044464235, 0.05524933483868613, 0.05524458767406786, 0.05523984893531189, 0.055235118606976955, 0.05523039667365595, 0.05522568311997588, 0.055220977930597534, 0.05521628109021546, 0.05521159258355772, 0.055206912395385714, 0.055202240510494154, 0.05519757691371069, 0.055192921589895944, 0.05518827452394329, 0.05518363570077869, 0.05517900510536053, 0.05517438272267951, 0.055169768537758505, 0.055165162535652366, 0.055160564701447826, 0.05515597502026334, 0.055151393477248956, 0.05514682005758613, 0.055142254746487665, 0.055137697529197546, 0.0551331483909908, 0.055128607317173346, 0.055124074293081866, 0.055119549304083755, 0.05511503233557687, 0.05511052337298952, 0.05510602240178027, 0.05510152940743782, 0.05509704437548093, 0.055092567291458276, 0.05508809814094826, 0.05508363690955906, 0.055079183582928334, 0.05507473814672324, 0.055070300586640204, 0.05506587088840496, 0.05506144903777224, 0.05505703502052585, 0.055052628822478474, 0.05504823042947156, 0.05504383982737522, 0.05503945700208816, 0.05503508193953754, 0.055030714625678864, 0.05502635504649593, 0.05502200318800065, 0.055017659036233034, 0.055013322577261034, 0.055008993797180404, 0.05500467268211479, 0.05500035921821539, 0.054996053391661005, 0.05499175518865794, 0.054987464595439836, 0.054983181598267664, 0.0549789061834296, 0.05497463833724084, 0.05497037804604376, 0.0549661252962075, 0.054961880074128146, 0.05495764236622849, 0.054953412158958034, 0.054949189438792796, 0.05494497419223534, 0.05494076640581462, 0.05493656606608597, 0.05493237315963089, 0.05492818767305708, 0.05492400959299837, 0.05491983890611449, 0.054915675599091204, 0.05491151965864005, 0.05490737107149833, 0.05490322982442909, 0.054899095904220915, 0.05489496929768798, 0.05489084999166989, 0.05488673797303166, 0.054882633228663574, 0.054878535745481176, 0.054874445510425175, 0.05487036251046136, 0.054866286732580524, 0.054862218163798465, 0.05485815679115577, 0.0548541026017179, 0.054850055582575, 0.05484601572084191, 0.0548419830036581, 0.054837957418187463, 0.05483393895161846, 0.0548299275911639, 0.054825923324060916, 0.05482192613757092, 0.05481793601897949, 0.05481395295559637, 0.05480997693475535, 0.05480600794381422, 0.05480204597015469, 0.05479809100118241, 0.05479414302432678, 0.054790202027040935, 0.05478626799680178, 0.05478234092110976, 0.05477842078748891, 0.0547745075834868, 0.054770601296674395, 0.05476670191464608, 0.05476280942501957, 0.054758923815435796, 0.05475504507355896, 0.05475117318707636, 0.0547473081436984, 0.05474344993115854, 0.0547395985372132, 0.054735753949641676, 0.0547319161562462, 0.05472808514485178, 0.054724260903306156, 0.05472044341947975, 0.054716632681265726, 0.05471282867657969, 0.0547090313933599, 0.054705240819567, 0.05470145694318413, 0.05469767975221677, 0.05469390923469267, 0.05469014537866194, 0.05468638817219682, 0.05468263760339178, 0.05467889366036329, 0.05467515633125, 0.05467142560421251, 0.05466770146743334, 0.054663983909116975, 0.054660272917489705, 0.05465656848079964, 0.05465287058731666, 0.05464917922533233, 0.05464549438315984, 0.054641816049134054, 0.054638144211611325, 0.05463447885896954, 0.05463081997960807, 0.05462716756194763, 0.05462352159443038, 0.054619882065519716, 0.054616248963700355, 0.05461262227747823, 0.05460900199538041, 0.054605388105955124, 0.054601780597771675, 0.05459817945942039, 0.05459458467951261, 0.05459099624668059, 0.05458741414957748, 0.0545838383768773, 0.054580268917274875, 0.05457670575948582, 0.05457314889224635, 0.054569598304313474, 0.0545660539844648, 0.054562515921498494, 0.05455898410423328, 0.054555458521508345, 0.05455193916218337, 0.05454842601513845, 0.05454491906927401, 0.05454141831351079, 0.054537923736789846, 0.054534435328072464, 0.0545309530763401, 0.054527476970594374, 0.05452400699985704, 0.05452054315316988, 0.05451708541959473, 0.054513633788213396, 0.054510188248127645, 0.05450674878845912, 0.05450331539834934, 0.054499888066959656, 0.05449646678347117, 0.054493051537084725, 0.05448964231702087, 0.05448623911251984, 0.05448284191284145, 0.054479450707265106, 0.05447606548508972, 0.054472686235633755, 0.054469312948235114, 0.0544659456122511, 0.05446258421705838, 0.05445922875205301, 0.05445587920665035, 0.05445253557028493, 0.05444919783241064, 0.05444586598250043, 0.054442540010046496, 0.05443921990456002, 0.0544359056555714, 0.054432597252629965, 0.05442929468530409, 0.054425997943181044, 0.05442270701586707, 0.05441942189298729, 0.05441614256418564, 0.05441286901912488, 0.05440960124748651, 0.054406339238970806, 0.05440308298329671, 0.054399832470201845, 0.054396587689442416, 0.05439334863079324, 0.05439011528404767, 0.054386887639017584, 0.054383665685533336, 0.0543804494134437, 0.054377238812615865, 0.05437403387293539, 0.054370834584306166, 0.05436764093665037, 0.054364452919908414, 0.05436127052403898, 0.05435809373901896, 0.05435492255484332]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def rmse(Y, Y_pred):\n",
        "\n",
        "    rmse = np.sqrt(np.mean((Y - Y_pred) ** 2))\n",
        "    return rmse"
      ],
      "metadata": {
        "id": "hoXGap0PJfTR"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def r2(Y, Y_pred):\n",
        "    mean_y = np.mean(Y)\n",
        "    ss_tot = np.sum((Y - mean_y) ** 2)  # Total Sum of Squares\n",
        "    ss_res = np.sum((Y - Y_pred) ** 2)  # Sum of Squared Residuals\n",
        "    r2 = 1 - (ss_res / ss_tot)  # Coefficient of Determination\n",
        "    return r2"
      ],
      "metadata": {
        "id": "-9cs_3Q-J1Fp"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    W = np.zeros(X_train.shape[1])\n",
        "    alpha = 0.00001\n",
        "    iterations = 1000\n",
        "\n",
        "    W_optimal, cost_history = gradient_descent(X_train, Y_train, W, alpha, iterations)\n",
        "\n",
        "    Y_pred = np.dot(X_test, W_optimal)\n",
        "\n",
        "    model_rmse = rmse(Y_test, Y_pred)\n",
        "    model_r2 = r2(Y_test, Y_pred)\n",
        "\n",
        "    print(\"Final Weights:\", W_optimal)\n",
        "    print(\"Cost History (First 10 iterations):\", cost_history[:10])\n",
        "    print(\"RMSE on Test Set:\", model_rmse)\n",
        "    print(\"R-Squared on Test Set:\", model_r2)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8X1TPkbTKbdf",
        "outputId": "246d6849-f40f-44d8-baf8-6ace7abbda98"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Weights: [0.34973248 0.64484523]\n",
            "Cost History (First 10 iterations): [2011.9142554734751, 1639.7141350925601, 1336.957481353757, 1090.6876393991413, 890.3653872086318, 727.4178946799879, 594.871837233527, 487.0549625191084, 399.35333835029707, 328.0138929475419]\n",
            "RMSE on Test Set: 3.749953052183746\n",
            "R-Squared on Test Set: -0.31832636502333056\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Did your Model Overfitt, Underfitts, or performance is acceptable.\n",
        "- The model underfits as the negative 𝑅^2 (-0.318) and high RMSE (3.75) show poor prediction accuracy. A low learning rate limits improvement, and better feature selection or tuning is needed."
      ],
      "metadata": {
        "id": "2Ell6y-2OHov"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(X, Y, learning_rate=0.01, iterations=1000, epsilon=1e-10):\n",
        "    m = len(Y)\n",
        "    W = np.zeros(X.shape[1])\n",
        "    cost_history = []\n",
        "\n",
        "    for i in range(iterations):\n",
        "        predictions = np.dot(X, W)\n",
        "        error = predictions - Y\n",
        "\n",
        "        if np.any(np.abs(error) > 1e10):\n",
        "            print(\"Warning: Large error values detected. Stopping gradient descent.\")\n",
        "            break\n",
        "\n",
        "        gradient = (1/m) * np.dot(X.T, error)\n",
        "        W -= learning_rate * gradient\n",
        "\n",
        "        cost = (1/(2*m)) * np.sum(error ** 2)\n",
        "\n",
        "        if np.isnan(cost):\n",
        "            print(\"Warning: Cost became NaN. Stopping gradient descent.\")\n",
        "            break\n",
        "\n",
        "        cost_history.append(cost)\n",
        "\n",
        "    return W, cost_history\n",
        "\n",
        "learning_rates = [0.0001, 0.001, 0.01, 0.1, 1]\n",
        "results = {}\n",
        "\n",
        "for lr in learning_rates:\n",
        "    W_optimal, _ = gradient_descent(X_train.values, Y_train.values, learning_rate=lr, iterations=1000)\n",
        "    Y_pred = np.dot(X_test, W_optimal)\n",
        "    Y_pred = np.nan_to_num(Y_pred, nan=0.0)\n",
        "    rmse = np.sqrt(mean_squared_error(Y_test, Y_pred))\n",
        "    r2 = r2_score(Y_test, Y_pred)\n",
        "    results[lr] = {'RMSE': rmse, 'R²': r2}\n",
        "\n",
        "for lr, result in results.items():\n",
        "    print(f\"Learning Rate: {lr}\")\n",
        "    print(f\"RMSE: {result['RMSE']}\")\n",
        "    print(f\"R²: {result['R²']}\")\n",
        "    print('-' * 40)\n"
      ],
      "metadata": {
        "id": "aVmqHmKXLdyl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34ee9167-2bfe-425e-9915-923a3616573c"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Large error values detected. Stopping gradient descent.\n",
            "Warning: Large error values detected. Stopping gradient descent.\n",
            "Warning: Large error values detected. Stopping gradient descent.\n",
            "Warning: Large error values detected. Stopping gradient descent.\n",
            "Learning Rate: 0.0001\n",
            "RMSE: 3.557387030877995\n",
            "R²: -0.1864064831992771\n",
            "----------------------------------------\n",
            "Learning Rate: 0.001\n",
            "RMSE: 1237496099.6745336\n",
            "R²: -1.4356843094153283e+17\n",
            "----------------------------------------\n",
            "Learning Rate: 0.01\n",
            "RMSE: 33406535864.02825\n",
            "R²: -1.046246848532443e+20\n",
            "----------------------------------------\n",
            "Learning Rate: 0.1\n",
            "RMSE: 3642529393.069695\n",
            "R²: -1.2438769105665638e+18\n",
            "----------------------------------------\n",
            "Learning Rate: 1\n",
            "RMSE: 3652574048702.0586\n",
            "R²: -1.2507466107423515e+24\n",
            "----------------------------------------\n"
          ]
        }
      ]
    }
  ]
}